---
title: "Extra Credit"
author: "Keyon Hedayati"
date: "4/27/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, load_packages}
library(tidyverse)
library(caret)
library(coefplot)
library(recipes)
library(visdat)
library(yardstick)
```

```{r, read_data_01}
df_all <- readr::read_csv("final_project_bonus.csv", col_names = TRUE)
```


```{r, summary}
df_all %>% summary()
```


```{r, categorical counts}
df_all %>% ggplot(mapping=aes(x=customer)) + geom_bar()
df_all %>% ggplot(mapping=aes(x=region)) + geom_bar()
df_all %>% ggplot(mapping=aes(x=outcome)) + geom_bar()
```


As you can see the data is massively imbalanced. Customer S and U barely account for anything, and the ratio of event to non-event is quite dramatic.

Lets see if we have any missing data and what it looks like.

```{r, missing}
visdat::vis_miss(df_all, cluster=TRUE) +
  theme(axis.text.x = element_text(size = 6.5, angle = 90))
```

Well that is some good news, nothing is missing. Lets look at customer via proportion

```{r, proportion}
df_all %>% 
  mutate(customer = forcats::fct_infreq(customer)) %>% 
  ggplot(mapping = aes(x = customer, y = stat(prop), group = 1)) +
  geom_bar() +
  coord_flip() +
  labs(x = "") +
  theme_bw()
```

Confirms what we already know, and verifies that G is also much higher than the rest. But lets look at our two categorical together in combination.

```{r}
df_all %>% 
  mutate(customer = forcats::fct_lump_prop(customer, 0.05),
         region = forcats::fct_lump_prop(region, 0.05)) %>% 
  count(customer, region) %>% 
  mutate(prop_total = n / sum(n)) %>% 
  ggplot(mapping = aes(x = customer, y = region)) +
  geom_tile(mapping = aes(fill = cut(prop_total,
                                     breaks = seq(0, 0.18, by = 0.03))),
            color = "black") +
  geom_text(mapping = aes(label = signif(prop_total, 3),
                          color = prop_total < 0.09)) +
  scale_fill_viridis_d("Proportion") +
  scale_color_manual(guide = 'none',
                     values = c("TRUE" = "white", 
                                "FALSE" = "black")) +
  theme_bw()
```


As you would suspect, not all customer groups exist in all regions. So an interaction between the two may not be the best idea. We should keep that in mind. The proportions are all wacky as well.

Even though we've already 'explored' the data, lets look at it again in relation to the outcome.

```{r, binary to numbers}
df_y <- df_all %>% mutate(y = ifelse(outcome == "event", 1, 0))
```


```{r, xa continous in & bi out | region/customer, fig.width=9, fig.height=7}

df_y %>% select(starts_with("xa"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=region)) +
  geom_jitter(height = 0.04) +
  facet_grid(region~name, scales = 'free')

df_y %>% select(starts_with("xa"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=customer)) +
  geom_jitter(height = 0.04) +
  facet_grid(customer~name, scales = 'free')

```


```{r, xb continous in & bi out | region/customer, fig.width=9, fig.height=7}

df_y %>% select(starts_with("xb"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=region)) +
  geom_jitter(height = 0.04) +
  facet_grid(region~name, scales = 'free')

df_y %>% select(starts_with("xb"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=customer)) +
  geom_jitter(height = 0.04) +
  facet_grid(customer~name, scales = 'free')

```



```{r, xn continous in & bi out | region/customer, fig.width=9, fig.height=7}

df_y %>% select(starts_with("xn"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=region)) +
  geom_jitter(height = 0.04) +
  facet_grid(region~name, scales = 'free')

df_y %>% select(starts_with("xn"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=customer)) +
  geom_jitter(height = 0.04) +
  facet_grid(customer~name, scales = 'free')

```

```{r, xw continous in & bi out | region/customer, fig.width=9, fig.height=7}

df_y %>% select(starts_with("xw"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=region)) +
  geom_jitter(height = 0.04) +
  facet_grid(region~name, scales = 'free')

df_y %>% select(starts_with("xw"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=customer)) +
  geom_jitter(height = 0.04) +
  facet_grid(customer~name, scales = 'free')

```


```{r, xs continous in & bi out | region/customer, fig.width=9, fig.height=7}

df_y %>% select(starts_with("xs"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=region)) +
  geom_jitter(height = 0.04) +
  facet_grid(region~name, scales = 'free')

df_y %>% select(starts_with("xs"), region, customer, response, y)  %>%  rowid_to_column()  %>% pivot_longer(!c("rowid", "region", "customer", "response", "y")) %>% 
  ggplot(mapping = aes(x=value, y=y, alpha=0.1, color=customer)) +
  geom_jitter(height = 0.04) +
  facet_grid(customer~name, scales = 'free')

```


Customers S and U have just a couple of data points for some of the features...

Lets make a model without accounting for class imbalance for now, so we can compare. Just a basic all additive.

```{r, control variables}
my_ctrl <- trainControl(method = 'cv', number = 5,
                             summaryFunction = twoClassSummary,
                             classProbs = TRUE,
                             savePredictions = TRUE)
my_metric <- "ROC"
```


```{r, default model}

default_model_all_add <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(response) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())

default_model_all_add %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names()

```


Lets train, but we will use elastic net at least

```{r, fit glm add all}
set.seed(98123)
fit_glm_add_all <- train(default_model_all_add, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

fit_glm_add_all
```

Without a tuning grid, it didn't perform that badly compared to the 'clean' data we worked with. But lets do some interactions with region and customer

```{r, region model}

default_model_region_X <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(customer, response) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~starts_with("region"):starts_with("x"))

default_model_region_X %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names()

```


```{r, fit glm region X}
set.seed(98123)
fit_glm_region_X <- train(default_model_region_X, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

fit_glm_region_X
```


Here we start to see a much lower ROC when we look at region interaction. But lets check out customer as well


```{r, customer model}

default_model_customer_X <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(region, response) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~starts_with("customer"):starts_with("x"))

default_model_customer_X %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, fit glm customer X}
set.seed(98123)
fit_glm_customer_X <- train(default_model_customer_X, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

fit_glm_customer_X
```
Notice we get a lot of warnings when running this model, because some of the factors are sparse like S.

We see the same diminished ROC values. We will use a new package that will help us with the upsampling in recipe called *themis*.

```{r, load themis}
library(themis)

```


First lets see if we can make improvements with lumping customers. We will lump everything with less than 5% proportion which will grab the two lowest.

```{r, customer model w lump}

lump_model_customer_X <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(region, response) %>% 
  step_other(customer, threshold = 0.05) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~starts_with("customer"):starts_with("x"))

lump_model_customer_X %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, fit glm customer X}
set.seed(98123)
lump_glm_customer_X <- train(lump_model_customer_X, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

lump_glm_customer_X
```


Now lets try with upsampling instead. We will upsample the lower class to 50%, so that it is 'picked' more often. We will try again with full 100

```{r, customer model w up 50}

upsample_50_model_customer_X <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(region, response) %>% 
  step_upsample(customer, over_ratio = 0.5) %>%
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~starts_with("customer"):starts_with("x"))

upsample_50_model_customer_X %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, up 50 glm customer X}
set.seed(98123)
up_50_glm_customer_X <- train(upsample_50_model_customer_X, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

up_50_glm_customer_X
```


```{r, customer model w up 100}

upsample_100_model_customer_X <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(region, response) %>% 
  step_upsample(customer, over_ratio = 1) %>%
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~starts_with("customer"):starts_with("x"))

upsample_100_model_customer_X %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, up 50 glm customer 100}
set.seed(98123)
up_100_glm_customer_X <- train(upsample_100_model_customer_X, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

up_100_glm_customer_X
```


Now lets take a look at the near zero variance features. It will remove sparse features are ones that are highly imbalanced.


```{r, customer model w nzv}

nzv_model_customer_X <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(region, response) %>% 
  step_nzv(all_predictors()) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~starts_with("customer"):starts_with("x"))

nzv_model_customer_X %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, nzv glm customer}
set.seed(98123)
nzv_glm_customer_X <- train(nzv_model_customer_X, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

nzv_glm_customer_X
```


Now that we have some powerful tools to help deal with imbalances, lets see what happens when we interact region AND customers. Lets try with near zero variance. There will be some features with no values, so even upsampling wont really work that well. But we can try near zero and see what happens.

```{r, customer X region w nzv}

nzv_model_customer_X_region <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(response) %>% 
  step_nzv(all_predictors()) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~starts_with("customer"):starts_with("region"):starts_with("x")) 


nzv_model_customer_X_region %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, nzv glm ustomer X region}
set.seed(98123)
nzv_glm_customer_X_region <- train(nzv_model_customer_X_region, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

nzv_glm_customer_X_region
```


Last but not least, lets just do all of the same sampling techniques for all additive, so we can compare some apples to apples.

```{r, all add up 50}

up_50_model_all_add <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(response) %>% 
  step_upsample(customer, over_ratio = .5) %>%
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) 
  


up_50_model_all_add %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, up 50 glm ustomer X region}
set.seed(98123)
up_50_glm_all_add <- train(up_50_model_all_add, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

up_50_glm_all_add
```

```{r, all add nzv}

nzv_model_all_add <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(response) %>% 
  step_nzv(all_predictors()) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) 
  


nzv_model_all_add %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, all add nzv glm}
set.seed(98123)
nzv_glm_all_add <- train(nzv_model_all_add, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

nzv_glm_all_add
```


Lets take a look at the results using visualizations.

```{r, roc viz}
all_cv_summary <- resamples(list(DEFAULT_ALL_ADD = fit_glm_add_all,
                                  DEFAULT_REGION_X = fit_glm_region_X,
                                  DEFAULT_CUSTOMER_X = fit_glm_customer_X,
                                  LUMP_OTHER_CUST_X = lump_glm_customer_X,
                                  UP_50_CUST_X = up_50_glm_customer_X,
                                  UP_100_CUST_X = up_100_glm_customer_X,
                                  NZV_CUSTOMER_X = nzv_glm_customer_X,
                                  NZV_CUST_X_REGION_X = nzv_glm_customer_X_region,
                                  UP_50_ALL_ADD = up_50_glm_all_add,
                                  NZV_ALL_ADD = nzv_glm_all_add
                                 ))


dotplot(all_cv_summary, metric = 'ROC')

```

Judging by the graphic up top, when it comes all additive, my upsample, or nzv performed about the same as the default, which is disappointing. When looking at customer interactions however, the significant improvement came from lumping only. Which does make some sense, since that is what the original data we've been working with was. The data was 'pre-lumped' into other, and ultimately that is what worked best. Which means I should try lumping all additive below.


```{r, all w lump}

lump_model_all_add <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(response) %>% 
  step_other(customer, threshold = 0.05) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) 

lump_model_all_add %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, fit glm all w lump}
set.seed(98123)
lump_glm_all_add <- train(lump_model_all_add, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

lump_glm_all_add
```

Lets try lump with nzv...


```{r, all w lump nzv}

lump_model_all_add_nzv <- recipe(outcome ~ .,
                       data = df_all) %>% 
  step_rm(response) %>% 
  step_nzv(all_numeric_predictors()) %>% 
  step_other(customer, threshold = 0.05) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) 

lump_model_all_add_nzv %>% 
  prep(training = df_all, retain = TRUE) %>% 
  bake(new_data = NULL) %>% 
  names() %>% tail()

```


```{r, fit all lump nzv}
set.seed(98123)
lump_glm_all_add_nzv <- train(lump_model_all_add_nzv, data = df_all,
                 method = "glm",
                 metric = my_metric,
                 trControl = my_ctrl)

lump_glm_all_add_nzv
```


```{r, roc viz 2}
all_cv_summary_2 <- resamples(list(DEFAULT_ALL_ADD = fit_glm_add_all,
                                  LUMP_OTHER_ALL_ADD = lump_glm_all_add,
                                  LUMP_OTHER_ALL_ADD_NZV = lump_glm_all_add_nzv
                                  
                                 ))

dotplot(all_cv_summary_2, metric = 'ROC')

```

Well we see that lump and NZV doesn't work too well. Lets take a look at the predictions

```{r, model predictions}
model_pred_results <- fit_glm_add_all$pred %>% tibble::as_tibble() %>% 
  select(pred, obs, event, non_event, rowIndex, Resample) %>% 
  mutate(model_name = "DEFAULT_ADD_ALL") %>% 
  bind_rows(lump_glm_all_add$pred %>% tibble::as_tibble() %>% 
  select(pred, obs, event, non_event, rowIndex, Resample) %>% 
  mutate(model_name = "LUMP_ADD_ALL")) %>% 
  bind_rows(lump_glm_all_add_nzv$pred %>% tibble::as_tibble() %>% 
              select(pred, obs, event, non_event, rowIndex, Resample) %>% 
              mutate(model_name = "NZV_LUMP_ADD_ALL"))
  
```


```{r, roc viz}
model_pred_results %>% 
  group_by(model_name) %>% 
  roc_curve(obs, event) %>% 
  autoplot()
```

This confirms what we already know. I would facet by customer, but since we have grouped into 'other' it hardly seems worth it. Ultimately, I feel that 'other' grouping was the best way to go for this data.

